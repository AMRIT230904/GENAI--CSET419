{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "GAN Image Generation System for MNIST/Fashion-MNIST\n",
        "Step-by-step implementation with debugging\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Configuration and Input Parameters\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class to store all hyperparameters\"\"\"\n",
        "    def __init__(self):\n",
        "        self.dataset_choice = None\n",
        "        self.epochs = None\n",
        "        self.batch_size = None\n",
        "        self.noise_dim = None\n",
        "        self.learning_rate = None\n",
        "        self.save_interval = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.img_size = 28\n",
        "        self.channels = 1\n",
        "\n",
        "    def get_user_input(self):\n",
        "        \"\"\"Get configuration from user input\"\"\"\n",
        "        print(\"=\" * 60)\n",
        "        print(\"GAN IMAGE GENERATION SYSTEM\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Dataset choice\n",
        "        while True:\n",
        "            choice = input(\"\\nChoose dataset (mnist/fashion): \").lower().strip()\n",
        "            if choice in ['mnist', 'fashion']:\n",
        "                self.dataset_choice = choice\n",
        "                break\n",
        "            print(\"Invalid choice. Please enter 'mnist' or 'fashion'\")\n",
        "\n",
        "        # Epochs\n",
        "        while True:\n",
        "            try:\n",
        "                epochs = int(input(\"Enter number of epochs (recommended 30-100): \"))\n",
        "                if epochs > 0:\n",
        "                    self.epochs = epochs\n",
        "                    break\n",
        "                print(\"Epochs must be positive\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid integer\")\n",
        "\n",
        "        # Batch size\n",
        "        while True:\n",
        "            try:\n",
        "                batch = int(input(\"Enter batch size (recommended 64 or 128): \"))\n",
        "                if batch > 0:\n",
        "                    self.batch_size = batch\n",
        "                    break\n",
        "                print(\"Batch size must be positive\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid integer\")\n",
        "\n",
        "        # Noise dimension\n",
        "        while True:\n",
        "            try:\n",
        "                noise = int(input(\"Enter noise dimension (recommended 50 or 100): \"))\n",
        "                if noise > 0:\n",
        "                    self.noise_dim = noise\n",
        "                    break\n",
        "                print(\"Noise dimension must be positive\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid integer\")\n",
        "\n",
        "        # Learning rate\n",
        "        while True:\n",
        "            try:\n",
        "                lr = float(input(\"Enter learning rate (recommended 0.0002): \"))\n",
        "                if lr > 0:\n",
        "                    self.learning_rate = lr\n",
        "                    break\n",
        "                print(\"Learning rate must be positive\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid number\")\n",
        "\n",
        "        # Save interval\n",
        "        while True:\n",
        "            try:\n",
        "                interval = int(input(\"Enter save interval (e.g., 5 for every 5 epochs): \"))\n",
        "                if interval > 0:\n",
        "                    self.save_interval = interval\n",
        "                    break\n",
        "                print(\"Save interval must be positive\")\n",
        "            except ValueError:\n",
        "                print(\"Please enter a valid integer\")\n",
        "\n",
        "        print(f\"\\n✓ Configuration complete. Using device: {self.device}\")\n",
        "        self.print_config()\n",
        "\n",
        "    def print_config(self):\n",
        "        \"\"\"Print current configuration\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"CONFIGURATION SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Dataset: {self.dataset_choice.upper()}\")\n",
        "        print(f\"Epochs: {self.epochs}\")\n",
        "        print(f\"Batch Size: {self.batch_size}\")\n",
        "        print(f\"Noise Dimension: {self.noise_dim}\")\n",
        "        print(f\"Learning Rate: {self.learning_rate}\")\n",
        "        print(f\"Save Interval: {self.save_interval}\")\n",
        "        print(f\"Device: {self.device}\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: Generator Network\n",
        "# ============================================================================\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator Network: Converts random noise to images\n",
        "    Input: noise vector of size (noise_dim,)\n",
        "    Output: image of size (1, 28, 28)\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_dim, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "        # Calculate the size needed for reshaping\n",
        "        self.init_size = img_shape[1] // 4  # 28 // 4 = 7\n",
        "\n",
        "        # Linear layer to expand noise\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(noise_dim, 128 * self.init_size ** 2)\n",
        "        )\n",
        "\n",
        "        # Convolutional layers to upsample\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),  # 7 -> 14\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),  # 14 -> 28\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, img_shape[0], 3, stride=1, padding=1),\n",
        "            nn.Tanh()  # Output range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        z: noise vector (batch_size, noise_dim)\n",
        "        returns: generated images (batch_size, 1, 28, 28)\n",
        "        \"\"\"\n",
        "        out = self.fc(z)\n",
        "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
        "        img = self.conv_blocks(out)\n",
        "        return img\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: Discriminator Network\n",
        "# ============================================================================\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator Network: Classifies images as real or fake\n",
        "    Input: image of size (1, 28, 28)\n",
        "    Output: probability of being real (single value)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(img_shape[0], 32, 3, 2, 1),  # 28 -> 14\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.Conv2d(32, 64, 3, 2, 1),  # 14 -> 7\n",
        "            nn.ZeroPad2d((0, 1, 0, 1)),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1),  # 7 -> 4\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout2d(0.25),\n",
        "        )\n",
        "\n",
        "        # Calculate output size after conv layers\n",
        "        ds_size = 4\n",
        "        self.adv_layer = nn.Sequential(\n",
        "            nn.Linear(256 * ds_size ** 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        img: images (batch_size, 1, 28, 28)\n",
        "        returns: probability of being real (batch_size, 1)\n",
        "        \"\"\"\n",
        "        out = self.model(img)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        return validity\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: Pre-trained Classifier for Label Prediction\n",
        "# ============================================================================\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"Simple CNN classifier for MNIST/Fashion-MNIST\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: Data Loading\n",
        "# ============================================================================\n",
        "\n",
        "def load_dataset(config):\n",
        "    \"\"\"Load and prepare the dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Normalize to [-1, 1] to match Generator output\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    if config.dataset_choice == 'mnist':\n",
        "        dataset = datasets.MNIST(\n",
        "            root='./data',\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=transform\n",
        "        )\n",
        "        print(\"✓ MNIST dataset loaded\")\n",
        "    else:\n",
        "        dataset = datasets.FashionMNIST(\n",
        "            root='./data',\n",
        "            train=True,\n",
        "            download=True,\n",
        "            transform=transform\n",
        "        )\n",
        "        print(\"✓ Fashion-MNIST dataset loaded\")\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        drop_last=True\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Total samples: {len(dataset)}\")\n",
        "    print(f\"✓ Batches per epoch: {len(dataloader)}\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    return dataloader, dataset\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: Train Classifier (for label prediction)\n",
        "# ============================================================================\n",
        "\n",
        "def train_classifier(dataloader, config):\n",
        "    \"\"\"Train a classifier for label prediction\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TRAINING CLASSIFIER FOR LABEL PREDICTION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    classifier = Classifier().to(config.device)\n",
        "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    classifier.train()\n",
        "    num_epochs = 5\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (imgs, labels) in enumerate(dataloader):\n",
        "            imgs, labels = imgs.to(config.device), labels.to(config.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = classifier(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        acc = 100. * correct / total\n",
        "        print(f\"Classifier Epoch {epoch+1}/{num_epochs} | Loss: {total_loss/len(dataloader):.4f} | Acc: {acc:.2f}%\")\n",
        "\n",
        "    print(\"✓ Classifier training complete\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    return classifier\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: GAN Training\n",
        "# ============================================================================\n",
        "\n",
        "def train_gan(config, dataloader, classifier):\n",
        "    \"\"\"Main GAN training function\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STARTING GAN TRAINING\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs('generated_samples', exist_ok=True)\n",
        "    os.makedirs('final_generated_images', exist_ok=True)\n",
        "\n",
        "    # Initialize networks\n",
        "    img_shape = (config.channels, config.img_size, config.img_size)\n",
        "    generator = Generator(config.noise_dim, img_shape).to(config.device)\n",
        "    discriminator = Discriminator(img_shape).to(config.device)\n",
        "\n",
        "    print(f\"✓ Generator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
        "    print(f\"✓ Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\\n\")\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
        "    optimizer_D = optim.Adam(discriminator.parameters(), lr=config.learning_rate, betas=(0.5, 0.999))\n",
        "\n",
        "    # Loss function\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "\n",
        "    # Fixed noise for consistent visualization\n",
        "    fixed_noise = torch.randn(25, config.noise_dim).to(config.device)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training Progress:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "        d_accuracies = []\n",
        "\n",
        "        for i, (real_imgs, _) in enumerate(dataloader):\n",
        "            batch_size = real_imgs.size(0)\n",
        "            real_imgs = real_imgs.to(config.device)\n",
        "\n",
        "            # Labels for real and fake\n",
        "            real_labels = torch.ones(batch_size, 1).to(config.device)\n",
        "            fake_labels = torch.zeros(batch_size, 1).to(config.device)\n",
        "\n",
        "            # ---------------------\n",
        "            # Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # Real images\n",
        "            real_output = discriminator(real_imgs)\n",
        "            d_loss_real = adversarial_loss(real_output, real_labels)\n",
        "\n",
        "            # Fake images\n",
        "            noise = torch.randn(batch_size, config.noise_dim).to(config.device)\n",
        "            fake_imgs = generator(noise)\n",
        "            fake_output = discriminator(fake_imgs.detach())\n",
        "            d_loss_fake = adversarial_loss(fake_output, fake_labels)\n",
        "\n",
        "            # Total discriminator loss\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "            d_loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Calculate discriminator accuracy\n",
        "            real_acc = (real_output > 0.5).float().mean()\n",
        "            fake_acc = (fake_output < 0.5).float().mean()\n",
        "            d_acc = (real_acc + fake_acc) / 2\n",
        "\n",
        "            # ---------------------\n",
        "            # Train Generator\n",
        "            # ---------------------\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Generate fake images\n",
        "            noise = torch.randn(batch_size, config.noise_dim).to(config.device)\n",
        "            gen_imgs = generator(noise)\n",
        "\n",
        "            # Generator tries to fool discriminator\n",
        "            g_output = discriminator(gen_imgs)\n",
        "            g_loss = adversarial_loss(g_output, real_labels)\n",
        "\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Store losses\n",
        "            d_losses.append(d_loss.item())\n",
        "            g_losses.append(g_loss.item())\n",
        "            d_accuracies.append(d_acc.item() * 100)\n",
        "\n",
        "        # Calculate epoch averages\n",
        "        avg_d_loss = np.mean(d_losses)\n",
        "        avg_g_loss = np.mean(g_losses)\n",
        "        avg_d_acc = np.mean(d_accuracies)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs} | D_loss: {avg_d_loss:.2f} | D_acc: {avg_d_acc:.2f}% | G_loss: {avg_g_loss:.2f}\")\n",
        "\n",
        "        # Save generated samples at intervals\n",
        "        if (epoch + 1) % config.save_interval == 0:\n",
        "            generator.eval()\n",
        "            with torch.no_grad():\n",
        "                sample_imgs = generator(fixed_noise)\n",
        "                save_image(sample_imgs.data,\n",
        "                          f'generated_samples/epoch_{epoch+1:03d}.png',\n",
        "                          nrow=5, normalize=True)\n",
        "            generator.train()\n",
        "            print(f\"  → Saved samples to generated_samples/epoch_{epoch+1:03d}.png\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"✓ GAN TRAINING COMPLETE\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "    return generator\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: Generate Final Images and Predict Labels\n",
        "# ============================================================================\n",
        "\n",
        "def generate_final_images(generator, classifier, config):\n",
        "    \"\"\"Generate 100 final images and predict their labels\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"GENERATING FINAL IMAGES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    generator.eval()\n",
        "    classifier.eval()\n",
        "\n",
        "    num_images = 100\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate in batches\n",
        "        for i in range(0, num_images, config.batch_size):\n",
        "            batch_size = min(config.batch_size, num_images - i)\n",
        "            noise = torch.randn(batch_size, config.noise_dim).to(config.device)\n",
        "            imgs = generator(noise)\n",
        "            all_images.append(imgs)\n",
        "\n",
        "            # Predict labels\n",
        "            outputs = classifier(imgs)\n",
        "            predicted = outputs.argmax(dim=1)\n",
        "            all_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "        # Concatenate all images\n",
        "        all_images = torch.cat(all_images, dim=0)\n",
        "\n",
        "        # Save individual images\n",
        "        for i in range(num_images):\n",
        "            save_image(all_images[i],\n",
        "                      f'final_generated_images/image_{i+1:03d}.png',\n",
        "                      normalize=True)\n",
        "\n",
        "        # Save grid\n",
        "        save_image(all_images.data[:100],\n",
        "                  'final_generated_images/grid_100.png',\n",
        "                  nrow=10, normalize=True)\n",
        "\n",
        "    print(f\"✓ Generated {num_images} images saved to final_generated_images/\")\n",
        "    print(f\"✓ Grid visualization saved to final_generated_images/grid_100.png\")\n",
        "\n",
        "    # Print label distribution\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"LABEL DISTRIBUTION OF GENERATED IMAGES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    label_names = {\n",
        "        'mnist': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
        "        'fashion': ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    }\n",
        "\n",
        "    labels_count = np.bincount(all_labels, minlength=10)\n",
        "    names = label_names[config.dataset_choice]\n",
        "\n",
        "    for i, (name, count) in enumerate(zip(names, labels_count)):\n",
        "        percentage = (count / num_images) * 100\n",
        "        bar = '█' * int(percentage / 2)\n",
        "        print(f\"{i} ({name:12s}): {count:3d} ({percentage:5.1f}%) {bar}\")\n",
        "\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Step 1: Get configuration\n",
        "    config = Config()\n",
        "    config.get_user_input()\n",
        "\n",
        "    # Step 2: Load dataset\n",
        "    dataloader, dataset = load_dataset(config)\n",
        "\n",
        "    # Step 3: Train classifier for label prediction\n",
        "    classifier = train_classifier(dataloader, config)\n",
        "\n",
        "    # Step 4: Train GAN\n",
        "    generator = train_gan(config, dataloader, classifier)\n",
        "\n",
        "    # Step 5: Generate final images and predict labels\n",
        "    generate_final_images(generator, classifier, config)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ALL TASKS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nOutput Summary:\")\n",
        "    print(\"1. Training logs printed above\")\n",
        "    print(\"2. Generated samples saved in: generated_samples/\")\n",
        "    print(\"3. Final 100 images saved in: final_generated_images/\")\n",
        "    print(\"4. Label distribution printed above\")\n",
        "    print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9rpFZUF4Ww7",
        "outputId": "8b7e9b7e-fb71-49c3-99ef-8d81569fe191"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GAN IMAGE GENERATION SYSTEM\n",
            "============================================================\n",
            "\n",
            "Choose dataset (mnist/fashion): mnist\n",
            "Enter number of epochs (recommended 30-100): 50\n",
            "Enter batch size (recommended 64 or 128): 80\n",
            "Enter noise dimension (recommended 50 or 100): 50\n",
            "Enter learning rate (recommended 0.0002): 0.0002\n",
            "Enter save interval (e.g., 5 for every 5 epochs): 5\n",
            "\n",
            "✓ Configuration complete. Using device: cuda\n",
            "\n",
            "============================================================\n",
            "CONFIGURATION SUMMARY\n",
            "============================================================\n",
            "Dataset: MNIST\n",
            "Epochs: 50\n",
            "Batch Size: 80\n",
            "Noise Dimension: 50\n",
            "Learning Rate: 0.0002\n",
            "Save Interval: 5\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "LOADING DATASET\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 487kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.40MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.85MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ MNIST dataset loaded\n",
            "✓ Total samples: 60000\n",
            "✓ Batches per epoch: 750\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "TRAINING CLASSIFIER FOR LABEL PREDICTION\n",
            "============================================================\n",
            "Classifier Epoch 1/5 | Loss: 0.2980 | Acc: 90.77%\n",
            "Classifier Epoch 2/5 | Loss: 0.1171 | Acc: 96.55%\n",
            "Classifier Epoch 3/5 | Loss: 0.0945 | Acc: 97.25%\n",
            "Classifier Epoch 4/5 | Loss: 0.0834 | Acc: 97.51%\n",
            "Classifier Epoch 5/5 | Loss: 0.0730 | Acc: 97.86%\n",
            "✓ Classifier training complete\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "STARTING GAN TRAINING\n",
            "============================================================\n",
            "\n",
            "✓ Generator parameters: 542,465\n",
            "✓ Discriminator parameters: 392,833\n",
            "\n",
            "Training Progress:\n",
            "------------------------------------------------------------\n",
            "Epoch 1/50 | D_loss: 0.64 | D_acc: 63.47% | G_loss: 0.83\n",
            "Epoch 2/50 | D_loss: 0.62 | D_acc: 66.24% | G_loss: 0.90\n",
            "Epoch 3/50 | D_loss: 0.59 | D_acc: 69.11% | G_loss: 0.99\n",
            "Epoch 4/50 | D_loss: 0.56 | D_acc: 71.45% | G_loss: 1.05\n",
            "Epoch 5/50 | D_loss: 0.55 | D_acc: 72.22% | G_loss: 1.09\n",
            "  → Saved samples to generated_samples/epoch_005.png\n",
            "Epoch 6/50 | D_loss: 0.53 | D_acc: 73.80% | G_loss: 1.14\n",
            "Epoch 7/50 | D_loss: 0.53 | D_acc: 73.72% | G_loss: 1.18\n",
            "Epoch 8/50 | D_loss: 0.52 | D_acc: 74.73% | G_loss: 1.21\n",
            "Epoch 9/50 | D_loss: 0.51 | D_acc: 74.90% | G_loss: 1.25\n",
            "Epoch 10/50 | D_loss: 0.51 | D_acc: 74.60% | G_loss: 1.26\n",
            "  → Saved samples to generated_samples/epoch_010.png\n",
            "Epoch 11/50 | D_loss: 0.50 | D_acc: 75.53% | G_loss: 1.30\n",
            "Epoch 12/50 | D_loss: 0.50 | D_acc: 75.68% | G_loss: 1.31\n",
            "Epoch 13/50 | D_loss: 0.49 | D_acc: 76.60% | G_loss: 1.37\n",
            "Epoch 14/50 | D_loss: 0.48 | D_acc: 76.97% | G_loss: 1.40\n",
            "Epoch 15/50 | D_loss: 0.49 | D_acc: 76.06% | G_loss: 1.39\n",
            "  → Saved samples to generated_samples/epoch_015.png\n",
            "Epoch 16/50 | D_loss: 0.48 | D_acc: 77.13% | G_loss: 1.43\n",
            "Epoch 17/50 | D_loss: 0.48 | D_acc: 76.76% | G_loss: 1.45\n",
            "Epoch 18/50 | D_loss: 0.47 | D_acc: 77.88% | G_loss: 1.49\n",
            "Epoch 19/50 | D_loss: 0.47 | D_acc: 78.14% | G_loss: 1.46\n",
            "Epoch 20/50 | D_loss: 0.46 | D_acc: 78.36% | G_loss: 1.56\n",
            "  → Saved samples to generated_samples/epoch_020.png\n",
            "Epoch 21/50 | D_loss: 0.45 | D_acc: 78.88% | G_loss: 1.57\n",
            "Epoch 22/50 | D_loss: 0.45 | D_acc: 78.68% | G_loss: 1.58\n",
            "Epoch 23/50 | D_loss: 0.45 | D_acc: 78.97% | G_loss: 1.61\n",
            "Epoch 24/50 | D_loss: 0.45 | D_acc: 78.64% | G_loss: 1.64\n",
            "Epoch 25/50 | D_loss: 0.45 | D_acc: 79.31% | G_loss: 1.60\n",
            "  → Saved samples to generated_samples/epoch_025.png\n",
            "Epoch 26/50 | D_loss: 0.43 | D_acc: 80.25% | G_loss: 1.72\n",
            "Epoch 27/50 | D_loss: 0.45 | D_acc: 78.74% | G_loss: 1.70\n",
            "Epoch 28/50 | D_loss: 0.42 | D_acc: 81.19% | G_loss: 1.71\n",
            "Epoch 29/50 | D_loss: 0.44 | D_acc: 79.32% | G_loss: 1.74\n",
            "Epoch 30/50 | D_loss: 0.44 | D_acc: 79.64% | G_loss: 1.68\n",
            "  → Saved samples to generated_samples/epoch_030.png\n",
            "Epoch 31/50 | D_loss: 0.41 | D_acc: 81.41% | G_loss: 1.78\n",
            "Epoch 32/50 | D_loss: 0.43 | D_acc: 80.03% | G_loss: 1.76\n",
            "Epoch 33/50 | D_loss: 0.42 | D_acc: 80.38% | G_loss: 1.81\n",
            "Epoch 34/50 | D_loss: 0.41 | D_acc: 81.34% | G_loss: 1.83\n",
            "Epoch 35/50 | D_loss: 0.41 | D_acc: 81.10% | G_loss: 1.89\n",
            "  → Saved samples to generated_samples/epoch_035.png\n",
            "Epoch 36/50 | D_loss: 0.41 | D_acc: 81.26% | G_loss: 1.90\n",
            "Epoch 37/50 | D_loss: 0.41 | D_acc: 81.72% | G_loss: 1.89\n",
            "Epoch 38/50 | D_loss: 0.39 | D_acc: 81.96% | G_loss: 1.87\n",
            "Epoch 39/50 | D_loss: 0.41 | D_acc: 81.39% | G_loss: 1.91\n",
            "Epoch 40/50 | D_loss: 0.39 | D_acc: 82.37% | G_loss: 2.01\n",
            "  → Saved samples to generated_samples/epoch_040.png\n",
            "Epoch 41/50 | D_loss: 0.37 | D_acc: 83.56% | G_loss: 1.98\n",
            "Epoch 42/50 | D_loss: 0.39 | D_acc: 82.54% | G_loss: 2.02\n",
            "Epoch 43/50 | D_loss: 0.37 | D_acc: 83.04% | G_loss: 2.01\n",
            "Epoch 44/50 | D_loss: 0.39 | D_acc: 82.40% | G_loss: 2.08\n",
            "Epoch 45/50 | D_loss: 0.39 | D_acc: 82.36% | G_loss: 2.06\n",
            "  → Saved samples to generated_samples/epoch_045.png\n",
            "Epoch 46/50 | D_loss: 0.36 | D_acc: 84.03% | G_loss: 2.10\n",
            "Epoch 47/50 | D_loss: 0.37 | D_acc: 83.45% | G_loss: 2.14\n",
            "Epoch 48/50 | D_loss: 0.39 | D_acc: 82.06% | G_loss: 2.05\n",
            "Epoch 49/50 | D_loss: 0.37 | D_acc: 83.27% | G_loss: 2.14\n",
            "Epoch 50/50 | D_loss: 0.37 | D_acc: 83.62% | G_loss: 2.21\n",
            "  → Saved samples to generated_samples/epoch_050.png\n",
            "\n",
            "============================================================\n",
            "✓ GAN TRAINING COMPLETE\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "GENERATING FINAL IMAGES\n",
            "============================================================\n",
            "✓ Generated 100 images saved to final_generated_images/\n",
            "✓ Grid visualization saved to final_generated_images/grid_100.png\n",
            "\n",
            "============================================================\n",
            "LABEL DISTRIBUTION OF GENERATED IMAGES\n",
            "============================================================\n",
            "0 (0           ):  10 ( 10.0%) █████\n",
            "1 (1           ):  13 ( 13.0%) ██████\n",
            "2 (2           ):  10 ( 10.0%) █████\n",
            "3 (3           ):  13 ( 13.0%) ██████\n",
            "4 (4           ):  15 ( 15.0%) ███████\n",
            "5 (5           ):  10 ( 10.0%) █████\n",
            "6 (6           ):  10 ( 10.0%) █████\n",
            "7 (7           ):   9 (  9.0%) ████\n",
            "8 (8           ):   6 (  6.0%) ███\n",
            "9 (9           ):   4 (  4.0%) ██\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "ALL TASKS COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "Output Summary:\n",
            "1. Training logs printed above\n",
            "2. Generated samples saved in: generated_samples/\n",
            "3. Final 100 images saved in: final_generated_images/\n",
            "4. Label distribution printed above\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYL-zln8-DMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}